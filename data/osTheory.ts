import type { TheoryContent } from "@/lib/types";

export const OS_THEORY: TheoryContent[] = [
    /* ── Unit 1 ─────────────────────────────────────────── */
    {
        topicSlug: "os-fundamentals",
        topicName: "OS Fundamentals",
        unit: 1,
        subject: "OS",
        summary:
            "An **Operating System (OS)** is system software that acts as an intermediary between the user and computer hardware. Its primary goals are to make the computer convenient to use and to manage hardware resources efficiently.\n\nThe **kernel** is the core program that runs at all times on the computer. Everything else is either a system program (ships with the OS) or an application program. From the system perspective, the OS is a **resource allocator** (manages CPU, memory, I/O devices) and a **control program** (prevents errors and misuse).\n\nThe **bootstrap program** (firmware) is stored in ROM or EEPROM. It initialises registers, device controllers, and memory, then loads the OS kernel. After booting, the first user-space process launched is **init** (PID 1), which spawns all other system daemons and services.",
        keyPoints: [
            "OS = intermediary between user and hardware",
            "Kernel = the one program running at all times",
            "Bootstrap (firmware) stored in ROM/EEPROM loads the kernel",
            "init is the first user-space process (PID 1)",
            "OS acts as resource allocator AND control program",
            "System programs ship with the OS; application programs are user-installed"
        ],
        commonMistakes: [
            "Confusing the kernel with system programs — the kernel is always running, system programs are not",
            "Thinking bootstrap is stored in RAM — it's in ROM/EEPROM (non-volatile)",
            "Saying the shell is the first process — init is first, shell is launched later"
        ]
    },
    {
        topicSlug: "interrupts-io",
        topicName: "Interrupts & I/O",
        unit: 1,
        subject: "OS",
        summary:
            "**Interrupts** are the mechanism by which hardware devices and software notify the CPU that an event needs attention. When an interrupt occurs, the CPU saves its current state (registers + program counter), looks up the appropriate handler in the **interrupt vector** (an array of ISR addresses), and transfers control to that handler.\n\n**Hardware interrupts** are generated by devices (keyboard, disk, network). **Traps** (or exceptions) are software-generated interrupts caused by errors (division by zero) or system calls.\n\n**DMA (Direct Memory Access)** allows high-speed I/O devices to transfer data directly to/from main memory without CPU involvement. The CPU sets up the DMA controller (source, destination, byte count), then is free to do other work. When the transfer completes, the DMA controller raises an interrupt.\n\n**Polling** is an alternative where the CPU continuously checks device status bits — simpler but wastes CPU cycles.",
        keyPoints: [
            "Interrupt vector maps IRQ numbers to ISR addresses",
            "Hardware interrupts come from devices; traps come from software",
            "CPU saves registers + program counter on interrupt",
            "DMA frees the CPU during large data transfers",
            "Polling = CPU busy-waits checking device status",
            "Device controllers have local buffers for I/O staging"
        ],
        commonMistakes: [
            "Confusing traps with hardware interrupts — traps are software-generated",
            "Thinking DMA requires constant CPU attention — it frees the CPU",
            "Forgetting that the interrupt vector stores addresses, not data"
        ]
    },
    {
        topicSlug: "memory-storage",
        topicName: "Memory & Storage",
        unit: 1,
        subject: "OS",
        summary:
            "Computer storage is organized in a **hierarchy** based on speed and cost. From fastest to slowest: **Registers → Cache → RAM (Main Memory) → SSD/Disk → Optical/Tape**.\n\nMain memory (RAM/DRAM) is **volatile** — it loses contents when power is removed. It's directly accessible by the CPU. **Secondary storage** (disks, SSDs) extends main memory with large, nonvolatile capacity.\n\n**Caching** copies frequently-accessed data into a faster storage level. Cache is managed by hardware (CPU cache) or software (OS page cache). The **Von Neumann architecture** follows a Fetch → Decode → Execute cycle, with instructions and data stored in the same main memory.\n\nDRAM (Dynamic RAM) needs constant refreshing but is cheap and dense. SRAM (Static RAM, used in caches) is faster but more expensive.",
        keyPoints: [
            "Hierarchy: Registers > Cache > RAM > Disk (fastest to slowest)",
            "RAM is volatile; secondary storage is nonvolatile",
            "Caching = copying data to faster storage",
            "Von Neumann: Fetch-Decode-Execute cycle",
            "DRAM needs refreshing; SRAM is faster but costlier",
            "Secondary storage extends main memory with persistence"
        ],
        commonMistakes: [
            "Getting the hierarchy order wrong — registers are fastest, not cache",
            "Confusing volatile and nonvolatile — RAM is volatile",
            "Thinking caching permanently stores data — it's temporary"
        ]
    },
    {
        topicSlug: "multiprocessing",
        topicName: "Multiprocessing",
        unit: 1,
        subject: "OS",
        summary:
            "**Multiprocessor systems** have multiple CPUs sharing the bus, clock, memory, and peripherals. In **Symmetric Multiprocessing (SMP)**, all processors are peers performing all tasks. In **Asymmetric Multiprocessing**, one master CPU controls slave CPUs with specific roles.\n\n**Multi-core** processors have multiple computing cores on a single chip. **NUMA (Non-Uniform Memory Access)** gives each CPU its own local memory with fast access, while remote memory access is slower.\n\n**Clustered systems** combine multiple independent systems (each with its own OS) that work together, sharing storage via a SAN (Storage Area Network). **Blade servers** pack multiple processor boards into a single chassis.\n\nAdvantages: increased throughput, fault tolerance (graceful degradation). Disadvantages: increased OS complexity, shared memory contention, higher cost.",
        keyPoints: [
            "SMP: all processors are peers (most common today)",
            "Multi-core: multiple cores on one chip",
            "NUMA: non-uniform memory access — local fast, remote slow",
            "Clusters: multiple separate systems working together",
            "Blade servers: multiple boards in one chassis",
            "Lower throughput is NOT a disadvantage — throughput increases"
        ],
        commonMistakes: [
            "Saying lower throughput is a disadvantage — multiprocessor increases throughput",
            "Confusing clusters with multiprocessor — clusters use separate systems",
            "Mixing up SMP and asymmetric — SMP has peer CPUs, not master-slave"
        ]
    },
    {
        topicSlug: "os-services",
        topicName: "OS Services & Protection",
        unit: 1,
        subject: "OS",
        summary:
            "The OS provides services through **system calls** — the interface between user programs and OS functionality. When a system call executes, the CPU switches from **user mode to kernel mode** (dual-mode operation), handles the request, then returns to user mode.\n\n**Dual-mode operation** protects the OS from user programs: privileged instructions can only execute in kernel mode. A **mode bit** tracks the current mode (0 = kernel, 1 = user).\n\n**Timers** prevent processes from monopolising the CPU — the OS sets a countdown, and when it expires, control returns to the OS. **Multiprogramming** keeps the CPU busy by having a pool of jobs; when one waits for I/O, the OS switches to another. **Timesharing (multitasking)** extends this by rapidly switching between interactive users.\n\n**System programs** (file managers, compilers, status tools) ship with the OS and provide a convenient environment.",
        keyPoints: [
            "System calls = interface to OS services",
            "Dual-mode: user mode (restricted) and kernel mode (privileged)",
            "Timer prevents infinite loops / resource hogging",
            "Multiprogramming: CPU always has a job to execute",
            "Timesharing: rapid context switching for interactive users",
            "DMA allows high-speed I/O without CPU intervention"
        ],
        commonMistakes: [
            "Thinking system calls execute in user mode — they trigger a switch to kernel mode",
            "Confusing multiprogramming with multiprocessing — multiprogramming is one CPU, many jobs",
            "Saying video games are system programs — they are application programs"
        ]
    },
    {
        topicSlug: "computing-environments",
        topicName: "Computing Environments",
        unit: 1,
        subject: "OS",
        summary:
            "Modern computing spans diverse environments. **Client-server** systems have clients requesting services from servers. **Peer-to-peer (P2P)** systems let every node act as both client and server. **Cloud computing** offers services at three levels: **IaaS** (servers/storage), **PaaS** (development platforms), **SaaS** (applications over the internet).\n\n**Embedded systems** (automobiles, appliances) run dedicated OS with rigid real-time constraints and little/no user interface. **Handheld devices** are optimised for usability and battery life.\n\nThe Linux kernel uses specialized data structures: **red-black trees** (balanced BST), **linked lists**, **hash maps**, and **bitmaps** (representing resource status as binary strings).",
        keyPoints: [
            "P2P: devices act as both client and server",
            "Cloud: IaaS (infrastructure), PaaS (platform), SaaS (software)",
            "Embedded systems: rigid timing, minimal/no UI",
            "Linux kernel uses red-black trees, linked lists, bitmaps",
            "Bitmaps represent resource status as binary digits",
            "Handheld devices optimized for usability + battery life"
        ],
        commonMistakes: [
            "Confusing IaaS with SaaS — IaaS provides raw infrastructure, SaaS provides applications",
            "Thinking embedded systems have GUIs — most have little/no user interface",
            "Saying Linux uses AVL trees — it uses red-black trees"
        ]
    },

    /* ── Unit 2 ─────────────────────────────────────────── */
    {
        topicSlug: "processes",
        topicName: "Processes",
        unit: 2,
        subject: "OS",
        summary:
            "A **process** is a program in execution. Its memory layout has four sections: **text** (code), **data** (global variables), **heap** (dynamically allocated memory), and **stack** (function call frames).\n\nA process cycles through states: **New → Ready → Running → Waiting → Terminated**. The **Process Control Block (PCB)** stores the process state, program counter, CPU registers, memory info, and scheduling data.\n\nThe **CPU scheduler** selects processes from the **ready queue**. **Context switching** saves the state of the current process to its PCB and loads the next process's state. The **fork()** system call creates a child process (a copy of the parent). **exec()** replaces the child's process image.\n\nA **zombie process** has finished executing but its entry remains in the process table (parent hasn't called wait()). An **orphan process** runs after its parent has terminated.",
        keyPoints: [
            "Process = program in execution (text + data + heap + stack)",
            "States: New → Ready → Running → Waiting → Terminated",
            "PCB stores state, PC, registers, memory info",
            "fork() creates a child (copy of parent); returns child PID to parent, 0 to child",
            "exec() replaces the process image — code after exec() is never reached if successful",
            "Zombie: finished but not waited on; Orphan: parent terminated"
        ],
        commonMistakes: [
            "Confusing a program with a process — a program is passive (on disk), a process is active",
            "Thinking fork() returns the same value to parent and child",
            "Forgetting that exec() replaces the entire process image"
        ]
    },
    {
        topicSlug: "ipc",
        topicName: "Inter-Process Communication",
        unit: 2,
        subject: "OS",
        summary:
            "**Cooperating processes** need IPC (Inter-Process Communication). The two fundamental models are **shared memory** (fastest — no kernel copying) and **message passing** (easier, better for distributed systems).\n\nThe **Producer-Consumer problem** is the classic IPC scenario. A **bounded buffer** has a fixed size; it's full when `(in+1)%BUFFER_SIZE == out`. An **unbounded buffer** has no practical size limit.\n\n**Pipes** are the simplest IPC: **ordinary (anonymous) pipes** are unidirectional (half-duplex) and require a parent-child relationship. **Named pipes (FIFOs)** have filesystem names and work between unrelated processes.\n\nIn **direct communication**, processes name each other explicitly. In **indirect communication**, messages go through **mailboxes (ports)**. Sends can be **blocking** (synchronous — waits until received) or **non-blocking** (asynchronous).\n\n**Shared memory** is fast because no data copying occurs, but requires explicit synchronisation (semaphores/mutexes).",
        keyPoints: [
            "Two IPC models: shared memory (fast) and message passing (easier)",
            "Bounded buffer full: (in+1) % BUFFER_SIZE == out",
            "Anonymous pipes: half-duplex, parent-child only",
            "Named pipes (FIFOs): filesystem name, any processes",
            "Direct communication: processes name each other",
            "Indirect communication: through mailboxes/ports",
            "Blocking send: waits until message received"
        ],
        commonMistakes: [
            "Thinking pipes are full-duplex — ordinary pipes are half-duplex",
            "Confusing bounded buffer full vs empty — full is (in+1)%SIZE==out, empty is in==out",
            "Forgetting shared memory needs explicit synchronization"
        ]
    },
    {
        topicSlug: "threads",
        topicName: "Threads & Concurrency",
        unit: 2,
        subject: "OS",
        summary:
            "A **thread** is the fundamental unit of CPU utilization — lighter-weight than a process. Threads within a process share **code, data, and open files** but have their own **stack, registers, and thread ID**.\n\n**Concurrency** = multiple tasks making progress (may interleave on one core). **Parallelism** = tasks running simultaneously on multiple cores.\n\n**Threading models**: **Many-to-One** (many user threads → one kernel thread; if one blocks, all block). **One-to-One** (each user thread → kernel thread; true parallelism but more overhead). **Many-to-Many** (flexible multiplexing).\n\n**Amdahl's Law**: Speedup = 1 / (S + (1−S)/N) where S = serial fraction, N = cores. This limits gains from adding cores.\n\n**Thread pools** pre-create threads to avoid creation/destruction overhead. **Signals** notify processes of events: synchronous (from the process itself) or asynchronous (external).\n\n**User threads** are managed by a user library; **kernel threads** are managed by the OS.",
        keyPoints: [
            "Thread = lightweight unit of CPU utilization",
            "Threads share code/data/files, own stack/registers",
            "Concurrency ≠ parallelism — concurrency is interleaving, parallelism is simultaneous",
            "One-to-One model: true parallelism, most common today",
            "Amdahl's Law: serial fraction limits speedup",
            "Thread pools avoid overhead of creating/destroying threads on demand",
            "Signals: synchronous (SIGSEGV) vs asynchronous (SIGINT)"
        ],
        commonMistakes: [
            "Confusing concurrency with parallelism",
            "Thinking Many-to-One allows true parallelism — it doesn't (one kernel thread)",
            "Applying Amdahl's Law incorrectly — if S=1 (fully serial), no speedup from cores"
        ]
    },
    {
        topicSlug: "synchronization",
        topicName: "Synchronization",
        unit: 2,
        subject: "OS",
        summary:
            "When multiple processes/threads access shared data, **race conditions** can occur. The **critical section problem** requires three guarantees: **Mutual Exclusion** (only one process in CS at a time), **Progress** (selection of next process happens in finite time), **Bounded Waiting** (limit on how long a process waits).\n\n**Peterson's Solution** uses `flag[]` and `turn` variables — works for two processes but unreliable on modern CPUs without memory barriers.\n\n**Semaphores** are integer counters: `wait()` decrements (blocks if ≤ 0), `signal()` increments. A semaphore initialized to 1 acts as a **mutex** (binary semaphore). **Counting semaphores** allow N concurrent accesses.\n\n**Monitors** encapsulate shared data with automatic mutual exclusion — only one process can be active inside at a time. Condition variables (`wait()`, `signal()`) allow processes to suspend inside the monitor.\n\nThe **Dining Philosophers** problem demonstrates deadlock: all pick up left chopstick → circular wait.",
        keyPoints: [
            "Critical section: mutual exclusion + progress + bounded waiting",
            "Peterson's solution: flag[] + turn; works for 2 processes",
            "Semaphore: wait() decrements, signal() increments",
            "Mutex = binary semaphore (initialized to 1)",
            "Monitor = automatic mutual exclusion with condition variables",
            "In producer-consumer: acquire counting semaphore BEFORE mutex to avoid deadlock"
        ],
        commonMistakes: [
            "Swapping wait(mutex) and wait(empty) in producer-consumer — always acquire counting semaphore first",
            "Thinking Peterson's solution works reliably on modern multi-core CPUs without memory barriers",
            "Confusing monitors with semaphores — monitors provide automatic exclusion, semaphores require manual lock/unlock"
        ]
    },
    {
        topicSlug: "deadlocks",
        topicName: "Deadlocks",
        unit: 2,
        subject: "OS",
        summary:
            "**Deadlock** requires all four conditions simultaneously: (1) **Mutual Exclusion** — resources are non-shareable. (2) **Hold and Wait** — process holds resources while waiting for others. (3) **No Preemption** — resources can't be forcibly taken. (4) **Circular Wait** — cycle of processes waiting for each other.\n\n**Prevention** breaks one condition: request all resources at once (breaks hold-and-wait), impose total ordering (breaks circular wait).\n\n**Avoidance** uses the **Banker's Algorithm**: maintain a **safe state** where some sequence of process completions is possible. Need = Max − Allocation. Grant a request only if the resulting state is safe. Work vector initialized to Available; Finish[] initialized to false.\n\n**Resource Allocation Graph (RAG)**: request edge P→R, assignment edge R→P. No cycle = no deadlock. Cycle with single-instance resources = definite deadlock. Cycle with multi-instance = possible deadlock (needs further analysis).\n\n**Livelock**: processes keep changing state in response to each other without making progress.",
        keyPoints: [
            "4 conditions: mutual exclusion, hold-and-wait, no preemption, circular wait",
            "Prevention: break any one condition",
            "Total ordering prevents circular wait",
            "Requesting all resources at once prevents hold-and-wait",
            "Banker's Algorithm: Need = Max - Allocation; check if safe state exists",
            "RAG cycle + single-instance = deadlock; multi-instance = maybe deadlock",
            "Livelock ≠ deadlock — processes aren't blocked but make no progress"
        ],
        commonMistakes: [
            "Thinking a cycle in RAG always means deadlock — only with single-instance resources",
            "Forgetting there are FOUR conditions, all needed simultaneously",
            "Confusing livelock with deadlock — in livelock processes keep running but accomplish nothing"
        ]
    }
];
